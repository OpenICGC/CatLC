{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "amateur-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from unet_parts import *\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from utils import metrics\n",
    "import matplotlib.patches as mpatches\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supported-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-06-20h18m-unet_ortho_simple\n"
     ]
    }
   ],
   "source": [
    "name = datetime.now().strftime(\"%d-%m-%Hh%Mm\") + '-unet_ortho_simple'\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "silent-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = pd.read_excel('../dataset_ICCV/legend.xlsx', usecols=[0,1,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "opposed-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Formatter(object):\n",
    "    def __init__(self, im):\n",
    "        self.im = im\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        z = self.im.get_array()[int(y), int(x)]\n",
    "        return f\"{legend.iloc[z]['Name']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_dirs = {\n",
    "    'orthophoto': '/CatLCNet/ORTHO/ORTHO_CAT_2018_UTM_WGS84_31N_1m.tif',\n",
    "    'orthophotoIR': '/CatLCNet/ORTHO/ORTHOIR_CAT_2018.tif',\n",
    "    'landcover': '/CatLCNet/LC/LC_2018_UTM_WGS84_31N_1m.tif',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_borders = np.load('../dataset_ICCV/no_borders_960.npy', allow_pickle=True)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(no_borders)\n",
    "no_borders_train = no_borders[:(int)(0.6*len(no_borders))]\n",
    "no_borders_val = no_borders[(int)(0.6*len(no_borders)):(int)(0.8*len(no_borders))]\n",
    "no_borders_test = no_borders[(int)(0.8*len(no_borders)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improving-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalize images.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        tile, orthophoto, orthophotoIR, landcover = sample['tile'], sample['orthophoto'], sample['orthophotoIR'], sample['landcover']\n",
    "                \n",
    "        stats_orthophoto = np.load('../dataset_ICCV/stats_orthophotoRGB.npy',allow_pickle=True)\n",
    "        stats_orthophotoIR = np.load('../dataset_ICCV/stats_orthophotoIR.npy',allow_pickle=True)\n",
    "        \n",
    "        normalize_orthophoto = transforms.Normalize(*stats_orthophoto)\n",
    "        normalize_orthophotoIR = transforms.Normalize(*stats_orthophotoIR)\n",
    "           \n",
    "        orthophoto = normalize_orthophoto(orthophoto)\n",
    "        orthophotoIR = normalize_orthophotoIR(orthophotoIR)\n",
    "        \n",
    "        \n",
    "        return {'tile': tile,\n",
    "                'orthophoto': orthophoto,\n",
    "                'orthophotoIR': orthophotoIR,\n",
    "                'landcover': landcover}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        tile, orthophoto, orthophotoIR, landcover = sample['tile'], sample['orthophoto'], sample['orthophotoIR'], sample['landcover']\n",
    "        \n",
    "        orthophoto = orthophoto.astype(np.float32)\n",
    "        orthophotoIR = orthophotoIR.astype(np.float32)\n",
    "        \n",
    "        return {'tile': tile, 'orthophoto': torch.from_numpy(orthophoto), 'orthophotoIR': torch.from_numpy(orthophotoIR), 'landcover': torch.from_numpy(landcover-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crucial-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatalanDataset(Dataset):\n",
    "    def __init__(self, tiles_list, raster_dirs, shape=320, transform=None): #960, to train 320\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tiles_list (): Path to the csv file with annotations.\n",
    "            raster_dirs (): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.tiles_list = tiles_list\n",
    "        self.raster_dirs = raster_dirs\n",
    "        self.transform = transform\n",
    "        self.shape = shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "            \n",
    "        window_1m = Window(*self.tiles_list[idx], self.shape, self.shape)\n",
    "        \n",
    "        with rasterio.open(self.raster_dirs['orthophoto']) as src:\n",
    "            raster_orthophoto = src.read(window=window_1m) # (3, shape, shape) uint8\n",
    "            \n",
    "        with rasterio.open(self.raster_dirs['orthophotoIR']) as src:\n",
    "            raster_orthophotoIR = src.read(window=window_1m) # (3, shape, shape) uint8\n",
    "        \n",
    "        with rasterio.open(self.raster_dirs['landcover']) as src:\n",
    "            raster_landcover = src.read(window=window_1m) # (1,shape,shape) uint8\n",
    "            \n",
    "        sample = {'tile': idx, 'orthophoto': raster_orthophoto, 'orthophotoIR': raster_orthophotoIR, 'landcover': raster_landcover}\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advisory-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatLC_ortho_train = CatalanDataset(no_borders_train, raster_dirs, transform=transforms.Compose([ToTensor(), Normalize()]))\n",
    "CatLC_ortho_val = CatalanDataset(no_borders_val, raster_dirs, transform=transforms.Compose([ToTensor(), Normalize()]))\n",
    "CatLC_ortho_test = CatalanDataset(no_borders_test, raster_dirs, transform=transforms.Compose([ToTensor(), Normalize()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "competent-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CatLC_ortho_train, batch_size=16, shuffle=True, num_workers=20)\n",
    "val_loader = DataLoader(CatLC_ortho_val, batch_size=16, shuffle=True, num_workers=10)\n",
    "test_loader = DataLoader(CatLC_ortho_test, batch_size=1, shuffle=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "underlying-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "common-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continental-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=4, n_classes=41, bilinear=True)\n",
    "\n",
    "model = model.to(device);\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "african-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(name, epoch, model, optimizer, train_losses, val_losses):\n",
    "    PATH = f'saved_models/{name}'\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "useful-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "selected-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset_ICCV/cmap_mcsc.pickle', 'rb') as f:\n",
    "    cmap_mcsc = np.load(f, allow_pickle=True)\n",
    "with open('../dataset_ICCV/norm_mcsc.pickle', 'rb') as f:\n",
    "    norm_mcsc = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "listed-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure():\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "    data = next(iter(test_loader))\n",
    "\n",
    "    tile = no_borders_test[data['tile']]\n",
    "    fig, axs = plt.subplots(1,3, figsize=(14,5))\n",
    "\n",
    "    window = Window(*tile, 320, 320)\n",
    "    with rasterio.open(raster_dirs['orthophoto']) as src:\n",
    "        pic = src.read(window=window)\n",
    "    axs[0].imshow(np.moveaxis(pic,0,-1))\n",
    "    axs[0].set_title('Orthophoto')\n",
    "\n",
    "    with rasterio.open(raster_dirs['landcover']) as src:\n",
    "        pic_landcover = src.read(window=window) - 1\n",
    "    plot_landcover = axs[1].imshow(pic_landcover[0], cmap=cmap_mcsc, vmin=0, vmax=41, interpolation='none')\n",
    "    axs[1].set_title('landcover')\n",
    "    axs[1].format_coord = Formatter(plot_landcover)\n",
    "\n",
    "    input_data_highres = torch.cat((data['orthophoto'], data['orthophotoIR']), 1)\n",
    "\n",
    "    prediction = model(input_data_highres.to(device))\n",
    "    prediction = torch.max(prediction, 1)[1][0].cpu()\n",
    "    labels = data['landcover'].squeeze().long().cpu()\n",
    "\n",
    "    plot_prediction = axs[2].imshow(prediction, cmap=cmap_mcsc, vmin=0, vmax=41, interpolation='none')\n",
    "    axs[2].set_title(f'prediction. acc: {torch.sum(prediction==labels)/labels.nelement():.2f}, iou: {metrics.mean_iou(prediction, labels, 41):.2f}')\n",
    "    #axs[2].set_title(f'prediction, accuracy: {100*np.sum(torch.max(prediction, 1)[1].cpu() == pic_landcover[0])/pic_landcover.size:.2f}%')\n",
    "    axs[2].format_coord = Formatter(plot_prediction)\n",
    "\n",
    "    classes_in_images = np.unique([prediction.numpy(),labels.numpy()])\n",
    "    patches =[mpatches.Patch(color=cmap_mcsc.colors[i],label=legend.iloc[i]['Name']+f' ({i+1})') for i in classes_in_images]\n",
    "    fig.legend(handles=patches, loc='lower center', ncol = (len(patches) if len(patches) < 6 else 5))\n",
    "\n",
    "    fig.suptitle(f'Tile {tile}')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-crisis",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 247/1277 [03:44<15:29,  1.11it/s, batch_idx=246, Loss=1.54, Accuracy (%)=58.3, Learning rate=0.0001]"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 80):\n",
    "    loss_per_batch = torch.tensor((), dtype=torch.float32).new_empty(\n",
    "        (train_loader.__len__())\n",
    "    )\n",
    "    with tqdm(train_loader) as t:\n",
    "        for batch_idx, data in enumerate(t):\n",
    "\n",
    "            model.train()\n",
    "            # data\n",
    "            labels, inputs_highres = data['landcover'], torch.cat((data['orthophoto'], data['orthophotoIR']), 1)\n",
    "            \n",
    "            labels = torch.squeeze(labels.long(),1)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            inputs_highres = inputs_highres.to(device)\n",
    "\n",
    "            # forward\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs_highres)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            loss_per_batch[batch_idx] = loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            accuracy = 100 * torch.sum(torch.max(outputs, 1)[1] == labels)/labels.nelement()\n",
    "            \n",
    "            t.set_postfix({\"batch_idx\": batch_idx,\n",
    "                            \"Loss\": loss.item(),\n",
    "                            \"Accuracy (%)\": accuracy.item(),\n",
    "                            \"Learning rate\": optimizer.param_groups[0][\"lr\"],})\n",
    "            \n",
    "        loss_per_epoch = torch.mean(loss_per_batch)\n",
    "        t.set_postfix({\"Total loss\": loss_per_epoch.item()})\n",
    "        train_losses.append(loss_per_epoch.item())        \n",
    "            \n",
    "    loss_per_batch = torch.tensor((), dtype=torch.float32).new_empty(\n",
    "        (val_loader.__len__())\n",
    "    )            \n",
    "    with tqdm(val_loader) as t:\n",
    "        for batch_idx, data in enumerate(t):\n",
    "\n",
    "            model.eval()\n",
    "            # data\n",
    "            labels, inputs_highres = data['landcover'], torch.cat((data['orthophoto'], data['orthophotoIR']), 1)\n",
    "            \n",
    "            labels = torch.squeeze(labels.long(),1)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            inputs_highres = inputs_highres.to(device)\n",
    "\n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "\n",
    "                outputs = model(inputs_highres)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss.mean()\n",
    "\n",
    "                loss_per_batch[batch_idx] = loss.item()\n",
    "\n",
    "                accuracy = 100 * torch.sum(torch.max(outputs, 1)[1] == labels)/labels.nelement()\n",
    "\n",
    "                t.set_postfix({\"batch_idx\": batch_idx,\n",
    "                                \"Loss\": loss.item(),\n",
    "                                \"Accuracy (%)\": accuracy.item(),\n",
    "                                \"Learning rate\": optimizer.param_groups[0][\"lr\"],})\n",
    "\n",
    "        loss_per_epoch = torch.mean(loss_per_batch).item()\n",
    "        t.set_postfix({\"Total loss\": loss_per_epoch})\n",
    "        \n",
    "        if any([loss_per_epoch > i for i in val_losses]) and epoch != 0:\n",
    "            patience += 1\n",
    "        else:\n",
    "            patience = 0\n",
    "            save_model(name, epoch, model, optimizer, train_losses, val_losses)\n",
    "        \n",
    "        val_losses.append(loss_per_epoch)\n",
    "        \n",
    "    writer = SummaryWriter('logs/'+ name)\n",
    "    writer.add_figure('figure', plot_figure(), global_step=epoch);\n",
    "    writer.add_scalars('Loss', {'Train': train_losses[epoch], 'Validation': val_losses[epoch]}, global_step=epoch)\n",
    "    writer.close()\n",
    "    \n",
    "    if patience >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "lasting-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_collection = MetricCollection([\n",
    "    Accuracy(num_classes=41, average=None, mdmc_average='global'),\n",
    "    Precision(num_classes=41, average=None, mdmc_average='global'),\n",
    "    Recall(num_classes=41, average=None, mdmc_average='global'),\n",
    "    IoU(num_classes=41, reduction='none')\n",
    "])\n",
    "metric_collection.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bibliographic-arkansas",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6808/6808 [28:11<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with tqdm(test_loader) as t:\n",
    "    for batch_idx, data in enumerate(t):\n",
    "        with torch.no_grad():\n",
    "            labels, inputs_highres = data['landcover'], torch.cat((data['orthophoto'], data['orthophotoIR']), 1)            \n",
    "            labels = torch.squeeze(labels.long(),1)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            inputs_highres = inputs_highres.to(device)\n",
    "            \n",
    "            outputs = model(inputs_highres)\n",
    "            preds = torch.max(outputs, 1)[1]\n",
    "            \n",
    "            metrics = metric_collection(preds.cpu(), labels.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "convertible-grenada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': tensor([0.9035, 0.0379, 0.8146, 0.6660, 0.7038, 0.0163, 0.8498, 0.6661, 0.6803,\n",
       "         0.6029, 0.2578, 0.1351, 0.3726, 0.5039, 0.2712, 0.1665, 0.0804, 0.6883,\n",
       "         0.0000, 0.1388, 0.5506, 0.7757, 0.5711, 0.4678, 0.5905, 0.4989, 0.7360,\n",
       "         0.1561, 0.4084, 0.0018, 0.7972, 0.1523, 0.0358, 0.3572, 0.0000, 0.9351,\n",
       "         0.6625, 0.4846, 0.4849, 0.5029, 0.0000]),\n",
       " 'Precision': tensor([0.8722, 0.1804, 0.8153, 0.7494, 0.6602, 0.0710, 0.8099, 0.6643, 0.6549,\n",
       "         0.6962, 0.2738, 0.1722, 0.2095, 0.5642, 0.4393, 0.3711, 0.6436, 0.5912,\n",
       "         0.0000, 0.3899, 0.5287, 0.5764, 0.4476, 0.3582, 0.6151, 0.3962, 0.6351,\n",
       "         0.3559, 0.5271, 0.1378, 0.6431, 0.3164, 0.3867, 0.6734, 0.0000, 0.6082,\n",
       "         0.6349, 0.6908, 0.5271, 0.7013, 0.0000]),\n",
       " 'Recall': tensor([0.9035, 0.0379, 0.8146, 0.6660, 0.7038, 0.0163, 0.8498, 0.6661, 0.6803,\n",
       "         0.6029, 0.2578, 0.1351, 0.3726, 0.5039, 0.2712, 0.1665, 0.0804, 0.6883,\n",
       "         0.0000, 0.1388, 0.5506, 0.7757, 0.5711, 0.4678, 0.5905, 0.4989, 0.7360,\n",
       "         0.1561, 0.4084, 0.0018, 0.7972, 0.1523, 0.0358, 0.3572, 0.0000, 0.9351,\n",
       "         0.6625, 0.4846, 0.4849, 0.5029, 0.0000]),\n",
       " 'IoU': tensor([0.7979, 0.0324, 0.6877, 0.5447, 0.5166, 0.0134, 0.7085, 0.4984, 0.5008,\n",
       "         0.4773, 0.1531, 0.0819, 0.1549, 0.3627, 0.2015, 0.1299, 0.0770, 0.4664,\n",
       "         0.0000, 0.1140, 0.3694, 0.4940, 0.3350, 0.2545, 0.4311, 0.2834, 0.5173,\n",
       "         0.1217, 0.2989, 0.0018, 0.5526, 0.1146, 0.0338, 0.3044, 0.0000, 0.5836,\n",
       "         0.4797, 0.3982, 0.3379, 0.4142, 0.0000])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_collection.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
